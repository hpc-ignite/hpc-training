### **üìå ‡πÑ‡∏ü‡∏•‡πå Python ‡πÅ‡∏•‡∏∞ Script ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning LLaMA 3.2:1B ‡∏ö‡∏ô LANTA HPC**
---
‡∏Å‡∏≤‡∏£ Fine-Tuning **LLaMA 3.2:1B ‡∏´‡∏£‡∏∑‡∏≠ ‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡πÉ‡∏î ‡πÜ ‡∏ö‡∏ô LANTA HPC** ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á **3 ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å** ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

1. **`finetune_llama.py`** ‚Üí ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡∏≠‡∏£‡πå‡∏™‡πÇ‡∏Ñ‡πâ‡∏î‡∏†‡∏≤‡∏©‡∏≤ Python ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning ‡πÇ‡∏°‡πÄ‡∏î‡∏•  
2. **`run_finetuning.slurm`** ‚Üí ‡πÑ‡∏ü‡∏•‡πå Job Script ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏ú‡πà‡∏≤‡∏ô SLURM ‡∏ö‡∏ô LANTA  
3. **`convert_to_gguf.py`** ‚Üí ‡πÑ‡∏ü‡∏•‡πå Python ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö Ollama  

---

## **‚úÖ 1. ‡πÑ‡∏ü‡∏•‡πå `finetune_llama.py` (Fine-Tuning LLaMA)**
‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå **‡∏´‡∏•‡∏±‡∏Å** ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• LLaMA 3.2:1B** ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **QLoRA** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ VRAM

üìå **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `finetune_llama.py`**:
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import load_dataset

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ Tokenizer
model_name = "meta-llama/Meta-Llama-3-1B"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ 8-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    load_in_8bit=True,
    device_map="auto"
)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA Configuration
lora_config = LoraConfig(
    r=16,  
    lora_alpha=32,  
    target_modules=["q_proj", "v_proj"],  
    lora_dropout=0.1,  
    bias="none"
)
model = get_peft_model(model, lora_config)

# ‡πÇ‡∏´‡∏•‡∏î Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Fine-Tuning
dataset = load_dataset("json", data_files="hpc_ignite_dataset.jsonl")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
training_args = TrainingArguments(
    output_dir="./llama3-finetuned",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=50,
    fp16=True,  
    optim="adamw_torch"
)

# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"]
)

trainer.train()

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
model.save_pretrained("llama3-finetuned")
tokenizer.save_pretrained("llama3-finetuned")
```
üîπ **‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏∞‡πÑ‡∏£?**
- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LLaMA 3.2:1B ‡∏à‡∏≤‡∏Å **Hugging Face**
- ‡πÇ‡∏´‡∏•‡∏î **Dataset JSONL** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å
- ‡πÉ‡∏ä‡πâ **QLoRA** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ VRAM
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà `llama3-finetuned/`

---

## **‚úÖ 2. ‡πÑ‡∏ü‡∏•‡πå `run_finetuning.slurm` (‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå SLURM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô Fine-Tuning)**
LANTA ‡πÉ‡∏ä‡πâ **SLURM** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô **(batch job scheduling)** ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `.slurm` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡∏ö‡∏ô **A100 GPU** 

üìå **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `run_finetuning.slurm`**:
```bash
#!/bin/bash
#SBATCH --job-name=llama-finetune
#SBATCH --partition=gpu
#SBATCH --gres=gpu:a100:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --time=10:00:00
#SBATCH --output=llama_finetune.log

echo "üîπ ‡πÇ‡∏´‡∏•‡∏î Mamba Module"
module load Mamba/23.11.0-0

echo "üîπ ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Conda Environment"
conda activate /project/cb900907-hpctgn/envs/llama-finetune

echo "üîπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ Fine-Tuning"
python finetune_llama.py

echo "‚úÖ ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
```
üîπ **‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏∞‡πÑ‡∏£?**
- ‡∏à‡∏≠‡∏á **A100 GPU 1 ‡∏ï‡∏±‡∏ß**
- ‡πÉ‡∏ä‡πâ **64GB RAM** ‡πÅ‡∏•‡∏∞ **8 CPU Cores**
- ‡πÇ‡∏´‡∏•‡∏î **Mamba Module** ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ **Conda Environment**
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ `finetune_llama.py`
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Log ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå `llama_finetune.log`

üìå **‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ö‡∏ô LANTA ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:**
```bash
sbatch run_finetuning.slurm
```

---

## **‚úÖ 3. ‡πÑ‡∏ü‡∏•‡πå `convert_to_gguf.py` (‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Ollama)**
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Fine-Tuning ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß ‡∏ï‡πâ‡∏≠‡∏á **‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•** ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô **GGUF format** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Ollama ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ

üìå **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `convert_to_gguf.py`**:
```python
import os

# ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô GGUF
convert_command = "python convert-hf-to-gguf.py --model_path llama3-finetuned --output llama3-finetuned.gguf"

# ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
os.system(convert_command)

print("‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô GGUF ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
```
üîπ **‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏∞‡πÑ‡∏£?**
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ `convert-hf-to-gguf.py`
- ‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô `llama3-finetuned.gguf`

üìå **‡∏£‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏ö‡∏ô LANTA:**
```bash
python convert_to_gguf.py
```

---

## **‚úÖ 4. Deploy ‡∏ö‡∏ô Ollama**
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô GGUF ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Ollama ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Deploy

üìå **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

üìå **‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Ollama**
```bash
ollama create hpc_ignite -f llama3-finetuned.gguf
```

üìå **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•**
```bash
ollama run hpc_ignite "‡∏â‡∏±‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ HPC ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
```

---

## **üöÄ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á**
| **‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå** | **‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà** |
|-------------|------------|
| **`finetune_llama.py`** | ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• LLaMA** ‡∏î‡πâ‡∏ß‡∏¢ **QLoRA** |
| **`run_finetuning.slurm`** | ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô SLURM ‡∏ö‡∏ô LANTA** |
| **`convert_to_gguf.py`** | ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡πÅ‡∏õ‡∏•‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô GGUF** |

---

## **üìå ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î**
1Ô∏è‚É£ **‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏ó‡∏µ‡πà LANTA**  
```bash
scp finetune_llama.py run_finetuning.slurm convert_to_gguf.py username@lanta.thaisc.org:~
```
2Ô∏è‚É£ **‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏¥‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà LANTA**
```bash
ssh -X username@lanta.thaisc.org
```
3Ô∏è‚É£ **‡∏£‡∏±‡∏ô Fine-Tuning ‡∏ö‡∏ô SLURM**
```bash
sbatch run_finetuning.slurm
```
4Ô∏è‚É£ **‡∏£‡∏≠‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô GGUF**
```bash
python convert_to_gguf.py
```
5Ô∏è‚É£ **‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ Ollama**
```bash
ollama create hpc_ignite -f llama3-finetuned.gguf
```
6Ô∏è‚É£ **‡∏ó‡∏î‡∏™‡∏≠‡∏ö Agentic AI**
```bash
ollama run hpc_ignite "‡∏â‡∏±‡∏ô‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ HPC ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?"
```

---

## ** ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥‡∏ö‡∏ô GPU Nodes ‡∏î‡πâ‡∏ß‡∏¢ LLaMa-Factory**
**1. ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå llm-finetuning**
```bash
cd training/llm-finetuning
```
**2. ‡∏™‡πà‡∏á job script ‡πÑ‡∏õ‡∏£‡∏±‡∏ô‡∏ö‡∏ô LANTA**
```bash 
sbatch host.sh
```
**3. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô**
```bash 
myqueue
```
**4. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå log ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô job**
```bash 
cat llama_webui_*.log
```
**5. ‡πÄ‡∏õ‡∏¥‡∏î terminal ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞ copy ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤**
**6. ‡πÄ‡∏õ‡∏¥‡∏î browser ‡∏î‡πâ‡∏ß‡∏¢ URL ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î LLaMa-Factory**
```bash 
http://localhost:7860
```
